---
title: "GRADE-RR Dataset"
layout: default
---

<!-- About -->
<section class="bg-light" id="about">
  <div class="container">
    <div class="jumbotron jumbotron-fluid">
      <div class="container" align="center">
        <h2>
          GRADE-RR: Generate Realistic Animated Dynamic Environments for Robotics Research.
        </h2>
        <h4><i>Submitted to RAL</i></h4>
        <p>
          <a href="https://eliabntt.github.io/">Elia Bonetto</a><sup>1,2,*</sup>,&nbsp;
          Chenghao Xu<sup>1</sup>,&nbsp;
          <a href="https://aamirahmad.de/">Aamir Ahmad</a><sup>2,1</sup>,&nbsp;
          <br />
          <sup>1</sup>Max Planck Institute for Intelligent Systems&nbsp;&nbsp;
          <sup>2</sup>University of Stuttgart&nbsp;&nbsp;
          <br />
          <sup>*</sup> Corresponding Author
        </p>
        <h5>
          <a href="">[Paper]</a>&nbsp;
          <a href="">[Download Data]</a>&nbsp;
          <a href="">[Environment Generation Code]</a>&nbsp; </br>
          <a href="">[People Generation Code]</a>&nbsp;
          <a href="">[Data Generation Pipeline]</a>
          <a href="">[Data Processing Code]</a>
        </h5>
      </div>
    </div>




    <div class="row">
      <div class="col-lg-12">
        <video autoplay muted loop width="100%">
          <source src="{{ site.baseurl }}/static/videos/03x03_videoWall.webm" type="video/webm">
          <source src="{{ site.baseurl }}/static/videos/03x03_videoWall.mp4" type="video/mp4">
          <source src="{{ site.baseurl }}/static/videos/03x03_videoWall.mp4" type="video/mp4">
          Sorry, we cannot display the GRADE-RR video as
          your browser doesn't support HTML5 video.
        </video>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12">
        <h2 class="section-heading text-uppercase">News</h2>
        <h5 class="text-muted" style="text-align:left;">
          <ul>
            <li>Annoncing <a href="">GRADE-RR</a></li>
            <li>Watch the dataset's <a href="" target="_blank">trailer</a></li>
            <li>Watch our GTC 2023 <a href="" target="_blank">talk</a></li>
          </ul>
        </h5>
      </div>
    </div>


    <div class="section">

      <div class="container_img">
        <p align="center">
          <img border="0" src="static/img/teaser.png" style="width: 90%" />
        </p>
      </div>
      <div class="container">
        <div align="center">
          <h2>Our Mission</h2>
        </div>
        <hr />
        <p>
          With GRADE-RR we seek to introduce a pipeline to easily simulate robots in dynamic environments.
          This will simplify the robotics research by a great extent by allowing easy and precise data
          generation with groundtruth in photorealistic environments.
          The project is based on <a href="">IsaacSim</a>, <a href="">Omniverse</a> and the open source <a
            href="">USD</a> format.
          Our work focuses on three main aspects: i) environments, ii) animations, and iii) simulation setup, management
          and data generation.
          The pipeline is completely customizable, extendable and improvable for any use-case (indoor, outdoor,
          manipulation ...).
          We indeed encourage the reader to introduce open source contributions to the project.

          The data generated is solely based on publicly available datasets and tools such as: <a href="">Front3D</a>,
          <a href="">Google Scanned Objects</a>, <a href="">ShapeNet</a>, <a href="">Cloth3D</a>, <a href="">AMASS</a>,
          <a href="">SMPL</a> and <a href="">Blenderproc</a>
          We provide .... sequences generated by an handheld camera
        </p>
      </div>

    </div>

    <br />

    <div class="row">
      <div class="col-md-6">
        <h4 class="service-heading">Environments</h4>
        <p class="text-muted">
          While perception and processing capabilities of our robot increases, there's the need to increase also the
          quality of our simulated environments.
          This will enable research based on simulations that are more coherent with the real world that the robot will
          sense when deployed, closing the sim-to-real gap.
          Using Omniverse's <a href="">connectors</a> you can convert almost any environment to the USD format. This can
          then be imported directly in IsaacSim.
          Our <a href="">conversion tool</a>, based on BlenderProc, focuses on the Front3D dataset, the biggest
          available semantically-annotated indoor dataset with actual 3D meshes.
          However, as we show <a href="">here</a> we can easily import environments from other applications such as
          UnrealEngine.
          Find more information about this <a href="">here</a>.
        </p>
      </div>
      <div class="col-md-6">
        <h4 class="service-heading">Animations</h4>
        <p class="text-muted">
          Most of the robotics research is being carried out in <b>static</b> environments. This mainly because animated
          assets are difficult to simulate, place and manage.
          <!--Moreover, most computer-vision-based datasets focusing on animated humans are generated without taking into account roboticists' needs or lack some degree
          of realism.-->
          In our work, we focused mainly on animated clothed humans by converting and placing inside our environments <a
            href="">Cloth3D</a> animated assets.
          To do so, we implemented a simple to use <a href="">tool</a> that allows a streamlined conversion between SMPL
          mesh and clothes animation to the USD format.
          The animations are then placed in the simulation with an easy to understand technique.
          However, the dynamic in the scene is not only humans but there can be objects and animals. To showcase
          usability, and the power of the simulation environment,
          we also generate sequences with flying objects taken from Google Scanned Objects and ScanNet, and a Zebra open
          source animated asset.
          Find more information about this <a href="">here</a>.
        </p>
      </div>
    </div>
    <div class="row">
      <!-- udated column -->
      <div class="col-md-6">
        <h4 class="service-heading">Simulation control and data generation</h4>
        <p class="text-muted">
          The core of the project is the simulation setup, control and the data generation pipeline.
          Here, we seek to simplify the learning curve that exists once one approaches IsaacSim.
          This includes loading the environment, placing and control the robot, placing the animations, animate static
          flying objects,
          ROS integration (not mandatory), data logging and saving, semantic annotation, and fine simulation control in
          time, physics and rendering.
          The pipeline is fully customizable, e.g. by removing ROS, control the robot/camera without external software,
          adding noise etc.
          Based on the complexity of the environment and the on the number of lights performances can reach realtime
          processing (>30fps) with RTX rendering settings.
          Data can be saved both by using <code>rosbag</code> or the common <code>npy</code> files.
          Find more information about this <a href="">here</a>.
        </p>
      </div>

      <div class="col-md-6">
        <h4 class="service-heading">Data processing and evaluation</h4>
        <p class="text-muted">
          By default, the generated data is produced as groundtruth. This means that you can have groundtruth
          information about RGB, depth, semantics, bounding-boxes,
          instances, motion-vector (optical flow), and asset skeleton/vertices positions.
          However, realistic data is noisy. That is why we implemented a set of <a href="">tools</a> to post-process the
          data to add noise and fix some known issues of the simulation.
          Moreover, by design, noise can be added at simulation time by including the class directly into the simulation
          environment or by using the latest Isaac release.
          Furthermore, to evaluate the usability of the pipeline and of the generated data we performed extensive
          testing with various SLAM libraries, including increasing human detection capabilities of
          both Mask-RCNN and YOLOv5 on real world datasets.
          Finally, we developed a tool to replay any experiment in such a way one can generate data with newly
          added/modified sensors and different environment settings.
          Find more information about this <a href="">here</a>.
        </p>
      </div>

    </div>
  </div>
</section>

<section class="bg-light" id="known_issues">
  <div class="container">
    <div class="row">
      <div class="col-md-12 text-center">
        <h4>Known Issues</h4>
      </div>
      <div class="col-md-12">
        <p class="text-muted">
          During our generation we found some issues related to bugs in IsaacSim itself. We address all of them in
          various sections of our code.
          The most notable ones are: rosbag clock timing issue fixed <a href="">here</a>, 3D bounding boxes for humans
          not correctly generated fixed <a href="">here</a>, simulation that automatically changes the aperture settings
          of the camera fixed <a href="">here</a>.
          LiDAR and LRF misfire with dynamic objects can be fixed by updating the code to the latest release and using a
          rendering-based laser sensor (check our branch <a href="">here</a>).
        </p>
      </div>
    </div>
  </div>
</section>

<section class="bg-light" id="downloads">
  <div class="container">
    <div class="row">
      <div class="col-md-12 text-center">
        <h2 class="section-heading text-uppercase">Download</h2>
        <h3 class="section-subheading text-muted">Dataset publicly available for research purposes will be released upon
          acceptance</h3>
      </div>
    </div>
    <div class="row">
      <div class="col-md-12">
        <h4 class="section-subheading" id="downloadFiles">Download groundtruth files</h4>
        <hr />

        <p>
          <code>Rosbags</code> (RGB, depth, IMU, odometry, tf tree)
        </p>
        <p>
          <code>npy</code> (RGB, depth, instance segmentation, bounding boxes)
        </p>
        <p>
          <code>General</code> (USD of the environment, simulation settings, rng state, logs)
        </p>

        <h4 class="section-subheading" id="downloadFiles">Download environments and animations</h4>
        <hr />
        <p>
          <code>3DFront Environments</code> (only USD files, textures downloaded separately)
        </p>
        <p>
          <code>Extra environments</code> (USD and textures)
        </p>
        <p>
          <code>Cloth3D</code> (only USD files, textures downloaded separately)
        </p>
        <p>
          <code>AMASS-CMU</code> (only USD files, textures downloaded separately)
        </p>
        <p>
          <code>Zebra</code> animations (only USD files)
        </p>


        <h4 class="section-subheading">Pipelines</h4>
        Code to generate the data is available at various location depending on the scope. If it involves a dataset, we
        suppose that you accepted the related license and downloaded the data on your own.

        <p>
          <code>AMASS</code> animations
        </p>

        <p>
          <code>Cloth3D</code> animations
        </p>

        <p>
          <code>Front3D</code> homes
        </p>

        <p>
          <code>Isaac sim manager</code>
        </p>

        <p>
          <code>Robot 6DOF control</code>
        </p>

        <p>
          <code>Data processing and testing</code>
        </p>

      </div>
    </div>
  </div>
</section>

<section>
  <div class="container">
    <div class="row">
      <div class="col-md-12 text-center">
        <h2 class="section-heading text-uppercase">Publications</h2>

      </div>
    </div>

    <p>If you find our work useful please cite our paper:
      <a href="">PDF</a> or <a href="">Arxiv</a>:
    </p>

    <div class="position-relative">
      <div class="right-0 top-0 position-absolute">
      <clipboard-copy aria-label="Copy" class="ClipboardButton btn m-2 p-0 tooltipped-no-delay"
    data-copy-feedback="Copied!" data-tooltip-direction="w" tabindex="0" role="button" id="btn-copy-clip"
    data-clipboard-target="#paper1">
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true"
      className="octicon octicon-paste js-clipboard-clippy-icon m-2">
      <path fillRule="evenodd"
        d="M5.75 1a.75.75 0 00-.75.75v3c0 .414.336.75.75.75h4.5a.75.75 0 00.75-.75v-3a.75.75 0 00-.75-.75h-4.5zm.75 3V2.5h3V4h-3zm-2.874-.467a.75.75 0 00-.752-1.298A1.75 1.75 0 002 3.75v9.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 13.25v-9.5a1.75 1.75 0 00-.874-1.515.75.75 0 10-.752 1.298.25.25 0 01.126.217v9.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-9.5a.25.25 0 01.126-.217z">
      </path>
    </svg>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true"
      className="octicon octicon-check js-clipboard-check-icon color-text-success d-none m-2">
      <path fillRule="evenodd"
        d="M13.78 4.22a.75.75 0 010 1.06l-7.25 7.25a.75.75 0 01-1.06 0L2.22 9.28a.75.75 0 011.06-1.06L6 10.94l6.72-6.72a.75.75 0 011.06 0z">
      </path>
    </svg>
  </clipboard-copy>
</div>
    <pre>
<code id="paper1">@ARTICLE{Damen2022RESCALING, 
    title={Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100}, 
    author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Furnari, Antonino and Ma, Jian and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and Wray, Michael}, 
    journal   = {International Journal of Computer Vision (IJCV)}, 
    year      = {2022}, 
    volume = {130}, 
    pages = {33â€“55}, 
    Url       = {https://doi.org/10.1007/s11263-021-01531-2}
}</code>
  </pre>
</div>

  </div>

</section>

<section>
  <div class="row">
    <div class="col-md-12 text-center">
      <h4 class="section-subheading">Disclaimer </h4>
      <p>GRADE-RR was collected as a tool for research in robotics. The dataset may
        have unintended biases (including those of a societal, gender or racial nature).</p>
    </div>
  </div>

  <div class="row">
    <div class="col-md-12 text-center">
      <h4 class="section-subheading">Copyright <img alt=""
          style="border-width:1px;float:left;margin-right:15px;margin-bottom:0px;" src="" /></h4>
      <p>
        All datasets and benchmarks on this page are copyright by us and published under the <a rel="license" href="">
      </p>

      <p>For commercial licenses of GRADE-RR and any of its annotations, email us at <a href=""></a></p>
    </div>
  </div>
</section>

<!-- Team -->
<section class="bg-light" id="team">
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <h2 class="section-heading text-uppercase">The Team</h2>
        <div class="text-muted">
        </div>
      </div>
    </div>


    <div class="row">
      <div class="col-md-6">
        <div class="team-member">
          <a href="http://eliabntt.github.io">
            <img class="mx-auto rounded-circle" src="{{site.baseurl}}/static/img/profile/eb.jpg" />

            <h4>Elia Bonetto</h4>
          </a>
          <h5>Principal Investigator</h5>
          <h6 class="text-muted">Max Planck Institute for Intelligent Systems, Germany</h6>
        </div>
      </div>

    </div>


</section>


<script type="application/ld+json">
{
  "@context":"http://schema.org/",
  "@type":"Dataset",
  "name":"GRADE-RR dataset",
  "description":"",
  "url":"",
  "sameAs":"",
  "citation":"",
  "identifier": "",
  "keywords":[
  ],
  "creator":{
     "@type":"",
     "url": "https://grade-rr.github.io/",
     "name":"GRADE-RR Team",
     "contactPoint":{
        "@type":"ContactPoint",
        "contactType": "technical support",
        "email":"elia.bonetto@tue.mpg.de",
        "url":""
     }
  },
  "distribution":[
     {
        "@type":"DataDownload",
        "encodingFormat":"video/mp4",
        "contentUrl":""
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"image/jpeg",
        "contentUrl":""
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"text/csv",
        "contentUrl":""
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"application/octet-stream",
        "contentUrl":""
     }
  ],
  "license": ""
}
</script>