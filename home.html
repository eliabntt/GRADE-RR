---
title: "GRADE-RR Dataset"
layout: default
---

<!-- About -->
<section class="bg-light" id="about">
  <div class="container-fluid">
    <div class="jumbotron jumbotron-fluid" id="title-box">
      <div class="container-fluid" align="center">
        <h2>
          GRADE-RR: Generate Realistic Animated Dynamic Environments for Robotics Research.
        </h2>
        <h4><i>Submitted to RAL</i></h4>
        <p>
          <a href="https://eliabntt.github.io/">Elia Bonetto</a><sup>1,2,*</sup>,&nbsp;
          <a href="https://kyle-xu-001.github.io/">Chenghao Xu</a><sup>1</sup>,&nbsp;
          <a href="https://aamirahmad.de/">Aamir Ahmad</a><sup>2,1</sup>,&nbsp;
          <br />
          <sup>1</sup>Max Planck Institute for Intelligent Systems&nbsp;&nbsp;
          <sup>2</sup>University of Stuttgart&nbsp;&nbsp;
          <br />
          <sup>*</sup> Corresponding Author
        </p>
        <h5>
          <a href="">[Paper]</a>&nbsp;
          <!-- <a href="">[Download Data]</a>&nbsp; TODO FIXME -->
          <a href="https://github.com/eliabntt/BlenderProc/tree/working_branch">[Environment Generation Code]</a>&nbsp; </br>
          <a href="https://github.com/eliabntt/generate_people">[People Generation Code]</a>&nbsp;
          <a href="https://github.com/eliabntt/isaac_sim_manager">[Data Generation Pipeline]</a>
          <a href="https://github.com/robot-perception-group/GRADE-eval/">[Data Processing Code]</a>
        </h5>
      </div>
    </div>
</section>



  <div class="container-fluid">
    <div class="row">
      <div class="col-md-6 img mx-auto">
        <!-- <video autoplay muted loop width="100%"> -->
          <!-- <source src="{{ site.baseurl }}/static/videos/03x03_videoWall.webm" type="video/webm"> -->
          <!-- <source src="{{ site.baseurl }}/static/videos/03x03_videoWall.mp4" type="video/mp4"> -->
          <!-- <source src="{{ site.baseurl }}/static/videos/03x03_videoWall.mp4" type="video/mp4"> -->
          <!-- Sorry, we cannot display the GRADE-RR video as -->
          <!-- your browser doesn't support HTML5 video. -->
        <!-- </video> -->
          <img  src="{{ site.baseurl }}/static/img/prom.png" class="img-fluid"/>
      </div>
    </div>

    <br />

    <div class="row">
      <div class="col-lg-12">
        <h2 class="section-heading text-uppercase text-center">News</h2>
        <h5 class="text-muted" style="text-align:left;">
          <ul>
            <li>Annoncing <a href="">GRADE-RR</a></li>
            <li>Watch the dataset's <a href="" target="_blank">trailer</a></li>
            <li>Watch our GTC 2023 <a href="" target="_blank">talk</a></li>
          </ul>
        </h5>
      </div>
    </div>

</div>

  <div class="section">
      <div class="container-fluid">
        <div align="center">
          <h2>Our Mission</h2>
        </div>
        <hr />
        <p>
          With GRADE-RR we introduce a pipeline to easily simulate robots in photorealistic dynamic environments.

          This will simplify the robotics research by a great extent by allowing easy data generation and precise custom simulations.
          
          The project is based on <a href="https://developer.nvidia.com/isaac-sim">IsaacSim</a>, <a href="https://www.nvidia.com/en-us/omniverse/">Omniverse</a> and the open source <a
            href="https://github.com/PixarAnimationStudios/USD">USD</a> format.
          
          Our framework is easily expandable and customizable for any use case.

          Our work focuses on five main aspects: i) environments, ii) dynamic assets, iii) robot/camera setup and control, iv) simulation control, data processing and extra tools.
          Each one of our modules can be easily removed from the pipeline, or exchanged with custom implementations.

          Furthermore, using our framework we generate an indoor dynamic environment dataset, and showcase various scenarios such as heterogeneous multi-robot setup and outdoor video capturing.
          The generated data has been used to extensive test various SLAM libraries and synthetic data usability in human detection/segmentation task with both Mask-RCNN and YOLOv5.
          
          The data generated is solely based on publicly available datasets and tools such as: <a href="https://tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset">3D-Front</a>,
          <a href="https://research.google/resources/datasets/scanned-objects-google-research/">Google Scanned Objects</a>, <a href="https://shapenet.org/">ShapeNet</a>, <a href="https://chalearnlap.cvc.uab.cat/dataset/38/description/">Cloth3D</a>, <a href="https://amass.is.tue.mpg.de/">AMASS</a>,
          <a href="https://smpl.is.tue.mpg.de/">SMPL</a> and <a href="https://github.com/DLR-RM/BlenderProc/">BlenderProc</a>.
        </p>
      </div>

    </div>

    <br />
<section class="bg-light" id="pipeline">
  <div class="container-fluid">  
  <div class="col-md-12 mx-auto img text-center" style="max-width: 900px;">
      <h2>GRADE pipeline</h2>
        <img class="img-fluid" src="{{site.baseurl}}/static/img/pipeline.png">
    </div>

    <div class="row">
      <div class="col-md-12 img">
        <!-- <video autoplay muted loop width="100%"> -->
          <!-- <source src="{{ site.baseurl }}/static/videos/03x03_videoWall.webm" type="video/webm"> -->
          <!-- <source src="{{ site.baseurl }}/static/videos/03x03_videoWall.mp4" type="video/mp4"> -->
          <!-- <source src="{{ site.baseurl }}/static/videos/03x03_videoWall.mp4" type="video/mp4"> -->
          <!-- Sorry, we cannot display the GRADE-RR video as -->
          <!-- your browser doesn't support HTML5 video. -->
        <!-- </video> -->
      </div>
    </div>
  </div>
</section>

<section class="bg-light" id="generate">
  <div class="text-center"> 
    <h2>More informations</h2>
  </div>
  <div class="container-fluid">
    <div class="row">
      <div class="col-md-6">
        <h4 class="service-heading">Environments</h4>
        <p class="text-muted">
          While perception and processing capabilities of our robot increases, there's the need to increase also the
          quality of our simulated environments.
          This will enable research based on simulations that are more coherent with the real world that the robot will
          sense when deployed, closing the sim-to-real gap.
          Using Omniverse's <a href="https://docs.omniverse.nvidia.com/con_connect/con_connect/overview.html">connectors</a> you can convert almost any environment to the USD format. This can
          then be imported directly in IsaacSim.
          Our <a href="https://github.com/eliabntt/BlenderProc/tree/working_branch">conversion tool</a>, based on BlenderProc, focuses on the 3D-Front dataset, the biggest
          available semantically-annotated indoor dataset with actual 3D meshes.
          However, as we show in our work we can easily import environments from other applications such as UnrealEngine, or download environments from popular marketplaces such as <a href="https://sketchfab.com/feed">SketchFab</a>.
          Find more information on how you can easily convert an environment <a href="">here</a>.
        </p>
      </div>
      <div class="col-md-6">
        <h4 class="service-heading">Dynamic assets</h4>
        <p class="text-muted">
          Most of the robotics research is being carried out in <b>static</b> environments. This mainly because animated
          assets are difficult to simulate, place and manage.
          In our work, we focused mainly on animated clothed humans by converting and placing inside our environments <a
            href="https://chalearnlap.cvc.uab.cat/dataset/38/description/">Cloth3D</a> and <a href="https://amass.is.tue.mpg.de/">AMASS</a> animated assets.
          To do so, we implemented a simple to use <a href="https://github.com/eliabntt/generate_people">tool</a> that allows a streamlined conversion between SMPL
          mesh and clothes animation to the USD format. The tool is expandable to different body models (e.g. SMPL-X) or different data sources.

          The animations are then placed in the simulation with an easy to understand technique which can be easily exchanged with custom implementations.
          
          However, dynamic assets are not only humans but there can be also objects or animals. Luckly, you can easily simulate those tools.
          Indeed, <a href="">here</a> we show how you can use our method to generate sequences with flying objects or <a href="">here</a> with animated Zebras.
        </p>
      </div>
    </div>
    <div class="row">
      <div class="col-md-6">
        <h4 class="service-heading">Robot/camera setup and control</h4>
        <p class="text-muted">
          Any robot can be imported in the simulation. 
          You can control the robot in various ways with or without ROS. Non-physics enabled possibilities include control through teleporting and as a flying object.
          Physics-enabled ones include software in the loop, joint-based waypoints, pre-developed controllers (e.g. standard vehicles) and directly through joint commands.
          Indeed, since Isaac Sim does not provide fluid-dynamic simulation and frictionless perpendicular movement, we developed a custom
          virtual <a href="https://github.com/eliabntt/custom_6dof_joint_controller">6DOF joint controller</a> that works both with position and velocity setpoints. This allowed us to control both a drone and a 3-wheeled omnidirectional robot.
          
          Robot's sensors can be created at run-time, and dynamically changed, or pre-configured directly in the USD file.

          Data publishing can be controlled manually, therefore each sensor can have a custom publishing and failure rate.

          Since the simulator is ROS-enabled any sensor can be published manually with added noise and any custom topic can be added or listened.

          Check out how you can <a href="">control</a> and <a href="">setup</a> any robot.
        </p>
      </div>

      <div class="col-md-6">
        <h4 class="service-heading">Simulation control, data processing and extras</h4>
        <p class="text-muted">
          The simulation control include loading the environment, placing and control the robot, placing the animations, animate objects,
          integrate ROS, assets/materials/lights randomization, dynamically set simulation settings, and data saving.
          The pipeline is fully customizable, e.g. with new placement strategies, by using or not ROS, have software in the loop,
          add noise to data etc.
          Based on the complexity of the environment and the on the number of lights performances can reach realtime
          processing (>30fps) with RTX rendering settings.
          Find more information about this <a href="">here</a>.
          Generated information includes RGB, depth, semantics, 2D-3D bounding-boxes,
          semantic instances, motion-vector (optical flow), and asset skeleton/vertices positions.
          We implemented a set of <a href="">tools</a> to post-process the data to add noise and fix some known issues of the simulation.
          <a href="">Data visualization</a> and <a href="">semantic mapping</a> tools are already available.
          Finally, we developed a way to replay any experiment in such a way one can generate data with newly added/modified sensors and different environment settings.
          Find more information about this <a href="">here</a>.
        </p>
      </div>
    </div>
    </div>
  </div>
</section>


<section class="bg-light" id="carousel">
  <div id="carousel-main" class="carousel slide col-md-6 mx-auto container-fluid" data-ride="carousel">
    <ol class="carousel-indicators">
      <li data-target="#carousel-main" data-slide-to="0" class="active"></li>
      <li data-target="#carousel-main" data-slide-to="1"></li>
      <li data-target="#carousel-main" data-slide-to="2"></li>
      <li data-target="#carousel-main" data-slide-to="3"></li>
      <li data-target="#carousel-main" data-slide-to="4"></li>
      <li data-target="#carousel-main" data-slide-to="5"></li>
      <li data-target="#carousel-main" data-slide-to="6"></li>
      <li data-target="#carousel-main" data-slide-to="7"></li>
    </ol>
    <div class="carousel-inner">
      <div class="carousel-item active" data-bs-interval="2000">
        <img class="d-block img-fluid" src="{{site.baseurl}}/static/carousel/rgb_conv.jpg" alt="A sample image">
        <div class="carousel-caption d-none d-md-block">
          <p>Sample of groundtruth generated data</p>
        </div>
      </div>
      <div class="carousel-item" data-bs-interval="2000">
        <img class="d-block img-fluid" src="{{site.baseurl}}/static/carousel/depth_conv.jpg" alt="Depth">
        <div class="carousel-caption d-none d-md-block">
          <p>Depth</p>
        </div>
      </div>
      <div class="carousel-item" data-bs-interval="2000">
        <img class="d-block img-fluid" src="{{site.baseurl}}/static/carousel/SMPL.png" alt="SMPL">
        <div class="carousel-caption d-none d-md-block">
          <p>SMPL shapes</p>
        </div>
      </div>
      <div class="carousel-item" data-bs-interval="2000">
        <img class="d-block img-fluid" src="{{site.baseurl}}/static/carousel/motion.jpg" alt="Motion vector">
        <div class="carousel-caption d-none d-md-block">
          <p>Motion vector</p>
        </div>
      </div>
      <div class="carousel-item" data-bs-interval="2000">
        <img class="d-block img-fluid" src="{{site.baseurl}}/static/carousel/bbox2d_conv.jpg" alt="2D bounding boxes">
        <div class="carousel-caption d-none d-md-block">
          <p>2D bounding boxes</p>
        </div>
      </div>
      <div class="carousel-item" data-bs-interval="2000">
        <img class="d-block img-fluid" src="{{site.baseurl}}/static/carousel/normal.jpg" alt="Normals">
        <div class="carousel-caption d-none d-md-block">
          <p>Normals</p>
        </div>
      </div>
      <div class="carousel-item" data-bs-interval="2000">
        <img class="d-block img-fluid" src="{{site.baseurl}}/static/carousel/instance_conv.jpg" alt="Instance segmentation">
        <div class="carousel-caption d-none d-md-block">
          <p>Instance segmentation</p>
        </div>
      </div>
      <div class="carousel-item" data-bs-interval="2000">
        <img class="d-block img-fluid" src="{{site.baseurl}}/static/carousel/PromImg.png" alt="Another point of view">
        <div class="carousel-caption d-none d-md-block">
          <p>Another point of view with overlays</p>
        </div>
      </div>
    </div>
    <a class="carousel-control-prev" href="#carousel-main" role="button" data-slide="prev">
      <span class="carousel-control-prev-icon" aria-hidden="true"></span>
      <span class="sr-only">Previous</span>
    </a>
    <a class="carousel-control-next" href="#carousel-main" role="button" data-slide="next">
      <span class="carousel-control-next-icon" aria-hidden="true"></span>
      <span class="sr-only">Next</span>
    </a>
  </div>
</section>

<section class="bg-light" id="download">
  <div class="container-fluid">
    <div class="row">
      <div class="col-md-12 text-center">
        <h2 class="section-heading text-uppercase">Download</h2>
        <h3 class="section-subheading text-muted">Dataset publicly available for research purposes will be released upon
          acceptance. With all the publicly available informations you should be able to reproduce our data. If in doubt, please reach out to me.</h3>
      </div>
    </div>
    <!--div class="row">
      <div class="col-md-12">
        <h4 class="section-subheading" id="downloadFiles">Download groundtruth files</h4>
        <hr />

        <p>
          <code>Rosbags</code> (RGB, depth, IMU, odometry, tf tree)
        </p>
        <p>
          <code>npy</code> (RGB, depth, instance segmentation, bounding boxes)
        </p>
        <p>
          <code>General</code> (USD of the environment, simulation settings, rng state, logs)
        </p>

        <h4 class="section-subheading" id="downloadFiles">Download environments and animations</h4>
        <hr />
        <p>
          <code>3DFront Environments</code> (only USD files, textures downloaded separately)
        </p>
        <p>
          <code>Extra environments</code> (USD and textures)
        </p>
        <p>
          <code>Cloth3D</code> (only USD files, textures downloaded separately)
        </p>
        <p>
          <code>AMASS-CMU</code> (only USD files, textures downloaded separately)
        </p>
        <p>
          <code>Zebra</code> animations (only USD files)
        </p>


        <h4 class="section-subheading">Pipelines</h4>
        Code to generate the data is available at various location depending on the scope. If it involves a dataset, we
        suppose that you accepted the related license and downloaded the data on your own.

        <p>
          <code>AMASS</code> animations
        </p>

        <p>
          <code>Cloth3D</code> animations
        </p>

        <p>
          <code>Front3D</code> homes
        </p>

        <p>
          <code>Isaac sim manager</code>
        </p>

        <p>
          <code>Robot 6DOF control</code>
        </p>

        <p>
          <code>Data processing and testing</code>
        </p>

      </div>
    </div-->
  </div>
</section>

<section class="bg-light" id="known_issues">
  <div class="container-fluid">
    <div class="row">
      <div class="col-md-12 text-center">
        <h4>Known Issues</h4>
      </div>
      <div class="col-md-12">
        <p class="text-muted">
          During our generation we found some issues related to bugs in IsaacSim itself. We address all of them in
          various sections of our code.
          The most notable ones are: rosbag clock timing issue fixed <a href="">here</a>, 3D bounding boxes for humans
          not correctly generated fixed <a href="">here</a>, simulation that automatically changes the aperture settings
          of the camera fixed <a href="">here</a>.
          LiDAR and LRF misfire with dynamic objects can be fixed by updating the code to the latest release and using a
          rendering-based laser sensor (check our branch <a href="">here</a>).
        </p>
      </div>
    </div>
  </div>
</section>

<section>
  <div class="container-fluid">
    <div class="row">
      <div class="col-md-12 text-center">
        <h2 class="section-heading text-uppercase">Publications</h2>

      </div>
    </div>

    <p>If you find our work useful please cite our paper:
      <a href="">PDF</a> or <a href="">Arxiv</a>:
    </p>

    <div class="position-relative">
      <div class="right-0 top-0 position-absolute">
      <clipboard-copy aria-label="Copy" class="ClipboardButton btn m-2 p-0 tooltipped-no-delay"
    data-copy-feedback="Copied!" data-tooltip-direction="w" tabindex="0" role="button" id="btn-copy-clip"
    data-clipboard-target="#paper1">
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true"
      className="octicon octicon-paste js-clipboard-clippy-icon m-2">
      <path fillRule="evenodd"
        d="M5.75 1a.75.75 0 00-.75.75v3c0 .414.336.75.75.75h4.5a.75.75 0 00.75-.75v-3a.75.75 0 00-.75-.75h-4.5zm.75 3V2.5h3V4h-3zm-2.874-.467a.75.75 0 00-.752-1.298A1.75 1.75 0 002 3.75v9.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 13.25v-9.5a1.75 1.75 0 00-.874-1.515.75.75 0 10-.752 1.298.25.25 0 01.126.217v9.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-9.5a.25.25 0 01.126-.217z">
      </path>
    </svg>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true"
      className="octicon octicon-check js-clipboard-check-icon color-text-success d-none m-2">
      <path fillRule="evenodd"
        d="M13.78 4.22a.75.75 0 010 1.06l-7.25 7.25a.75.75 0 01-1.06 0L2.22 9.28a.75.75 0 011.06-1.06L6 10.94l6.72-6.72a.75.75 0 011.06 0z">
      </path>
    </svg>
  </clipboard-copy>
</div>
    <pre>
<code id="paper1">@ARTICLE{Damen2022RESCALING, 
    title={Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100}, 
    author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Furnari, Antonino and Ma, Jian and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and Wray, Michael}, 
    journal   = {International Journal of Computer Vision (IJCV)}, 
    year      = {2022}, 
    volume = {130}, 
    pages = {33–55}, 
    Url       = {https://doi.org/10.1007/s11263-021-01531-2}
}</code>
  </pre>
</div>

  </div>

</section>


<section class="bg-light" id="known_issues">
  <div class="container-fluid">
    <div class="row">
      <div class="col-md-12 text-center">
        <h4>Disclaimer</h4>
      </div>
      <div class="col-md-12">
        <p class="text-muted">
          GRADE-RR was collected as a tool for research in robotics. The dataset may
        have unintended biases (including those of a societal, gender or racial nature).
        </p>
      </div>
    </div>

    <div class="row">
      <div class="col-md-12 text-center">
        <h4>Copyright</h4>
      </div>
      <div class="col-md-12">
        <p class="text-muted">
        All datasets and benchmarks on this page are copyright by us and published under the <a rel="license" href="">
        </p>
        <p>For commercial licenses of GRADE-RR and any of its annotations, email us at <a href=""></a></p>
      </div>
    </div>
  </div>
</section>


<!-- Team -->
<section class="bg-light" id="team">
  <div class="container-fluid">
    <div class="row">
      <div class="col-lg-12 text-center">
        <h2 class="section-heading text-uppercase">The Team</h2>
        <div class="text-muted">
        </div>
      </div>
    </div>


    <div class="row">
      <div class="col-md-4">
        <div class="team-member">
          <a href="https://eliabntt.github.io">
            <img class="mx-auto rounded-circle" src="{{site.baseurl}}/static/img/profile/eb.jpg" />

            <h4>Elia Bonetto</h4>
          </a>
          <h5>Principal Investigator</h5>
          <h6 class="text-muted">Max Planck Institute for Intelligent Systems, Germany</h6>
          <h6 class="text-muted">Institute of Flight Mechanics and Controls, University of Stuttgart, Germany</h6>
          <h6 class="text-muted">University of Tübingen, Germany</h6>
        </div>
      </div>

      <div class="col-md-4">
        <div class="team-member">
            <a href="https://kyle-xu-001.github.io/">
            <img class="mx-auto rounded-circle" src="{{site.baseurl}}/static/img/profile/cx.jpg" />
            <h4>Chenghao Xu</h4>
            </a>
            <h6 class="text-muted">Delft University of Technology, Netherlands</h6>
            <h6 class="text-muted">Max Planck Institute for Intelligent Systems, Germany</h6>
          </div>
      </div>

    <div class="col-md-4">
      <div class="team-member">
        <a href="https://aamirahmad.de">
          <img class="mx-auto rounded-circle" src="{{site.baseurl}}/static/img/profile/aa.jpeg" />

          <h4>Aamir Ahmad</h4>
        </a>
        <h6 class="text-muted">Institute of Flight Mechanics and Controls, University of Stuttgart, Germany</h6>
        <h6 class="text-muted">Max Planck Institute for Intelligent Systems, Germany</h6>
      </div>
    </div>
  </div>

</section>


<script type="application/ld+json">
{
  "@context":"http://schema.org/",
  "@type":"Dataset",
  "name":"GRADE-RR dataset",
  "description":"",
  "url":"",
  "sameAs":"",
  "citation":"",
  "identifier": "",
  "keywords":[
  ],
  "creator":{
     "@type":"",
     "url": "https://grade-rr.github.io/",
     "name":"GRADE-RR Team",
     "contactPoint":{
        "@type":"ContactPoint",
        "contactType": "technical support",
        "email":"elia.bonetto@tue.mpg.de",
        "url":""
     }
  },
  "distribution":[
     {
        "@type":"DataDownload",
        "encodingFormat":"video/mp4",
        "contentUrl":""
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"image/jpeg",
        "contentUrl":""
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"text/csv",
        "contentUrl":""
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"application/octet-stream",
        "contentUrl":""
     }
  ],
  "license": ""
}
</script>